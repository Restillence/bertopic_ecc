{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42.\n",
      "Initializing FileHandler and TextProcessor...\n",
      "Reading index file...\n",
      "Creating ECC sample...\n",
      "First 10 files in directory: ['earnings_call_10053_1405785.txt', 'earnings_call_10053_3087559.txt', 'earnings_call_10053_3264115.txt', 'earnings_call_10053_3495328.txt', 'earnings_call_10053_3838748.txt', 'earnings_call_10053_3978736.txt', 'earnings_call_10053_4168241.txt', 'earnings_call_10053_4240123.txt', 'earnings_call_10053_4711496.txt', 'earnings_call_10053_4825244.txt']\n",
      "Sampling mode: random_company\n",
      "Starting to create ECC sample...\n",
      "Found 74 files for permco 16285\n",
      "Found 57 files for permco 51198\n",
      "Found 9 files for permco 16653\n",
      "Found 1 files for permco 57141\n",
      "Found 21 files for permco 52786\n",
      "Found 1 files for permco 37170\n",
      "Found 3 files for permco 962\n",
      "Found 17 files for permco 29846\n",
      "Found 1 files for permco 38460\n",
      "Found 4 files for permco 21330\n",
      "Found 2 files for permco 57189\n",
      "Found 11 files for permco 50183\n",
      "Found 11 files for permco 12191\n",
      "Found 75 files for permco 13521\n",
      "Found 74 files for permco 14931\n",
      "Found 42 files for permco 11290\n",
      "Found 41 files for permco 15564\n",
      "Found 7 files for permco 56459\n",
      "Found 34 files for permco 50855\n",
      "Found 8 files for permco 55032\n",
      "Extracting and processing relevant sections...\n",
      "'Presentation' section not found for call ID: earnings_call_29846_626788\n",
      "Attempting to extract text from start up to 'Questions and Answers' section...\n",
      "Extracted text up to 'Questions and Answers' for call ID: earnings_call_29846_626788\n",
      "'Presentation' section not found for call ID: earnings_call_38460_763792\n",
      "Attempting to extract text from start up to 'Questions and Answers' section...\n",
      "Extracted text up to 'Questions and Answers' for call ID: earnings_call_38460_763792\n",
      "Extraction and processing completed in 16.34 seconds.\n",
      "Using device: cpu\n",
      "Loading SentenceTransformer model: all-MiniLM-L12-v2...\n",
      "SentenceTransformer embedding model loaded successfully.\n",
      "Loading BERTopic model from D:/daten_masterarbeit/bertopic_model_dir_zeroshot_075minsim_huang_own...\n",
      "BERTopic model loaded successfully.\n",
      "Computing embeddings for new documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e41f0e645f541a6a6f1663d68e94920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings for 57627 documents in 1014.10 seconds.\n",
      "Transforming documents with the BERTopic model...\n",
      "Transformed documents in 0.29 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Adjust the path to include 'src' if it's not already in the system path\n",
    "current_dir = os.getcwd()\n",
    "if \"src\" not in current_dir:\n",
    "    src_path = os.path.abspath(os.path.join(current_dir, '..', 'src'))\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import custom modules\n",
    "from file_handling import FileHandler\n",
    "from text_processing import TextProcessor\n",
    "from utils import print_configuration\n",
    "\n",
    "# Step 1: Set a random seed for reproducibility\n",
    "random_seed = 42  # You can change this value as needed\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "print(f\"Random seed set to {random_seed}.\")\n",
    "\n",
    "# Step 2: Initialize FileHandler and TextProcessor, read index file, create ECC sample, and extract relevant sections\n",
    "# Load configuration from config.json\n",
    "config_file_path = r'C:\\Users\\nikla\\OneDrive\\Dokumente\\winfoMaster\\Masterarbeit\\bertopic_ecc\\config.json'  # Update this path as needed\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "# Extract necessary variables from configuration\n",
    "index_file_ecc_folder = config.get(\"index_file_ecc_folder\")\n",
    "folderpath_ecc = config.get(\"folderpath_ecc\")\n",
    "sample_size = config.get(\"sample_size\")\n",
    "document_split = config.get(\"document_split\")\n",
    "section_to_analyze = config.get(\"section_to_analyze\")\n",
    "max_documents = config.get(\"max_documents\")\n",
    "model_load_path = config.get(\"model_load_path\")\n",
    "embedding_model_choice = config.get(\"embedding_model_choice\")\n",
    "output_dir = config.get(\"output_dir\", \"transformation_results\")\n",
    "\n",
    "# Initialize FileHandler and TextProcessor\n",
    "print(\"Initializing FileHandler and TextProcessor...\")\n",
    "file_handler = FileHandler(config=config)\n",
    "text_processor = TextProcessor(method=document_split, section_to_analyze=section_to_analyze)\n",
    "\n",
    "# Read the index file\n",
    "print(\"Reading index file...\")\n",
    "index_file = file_handler.read_index_file()\n",
    "\n",
    "# Create ECC sample\n",
    "print(\"Creating ECC sample...\")\n",
    "ecc_sample = file_handler.create_ecc_sample(sample_size)\n",
    "\n",
    "# Extract texts for BERTopic analysis (processed sections/paragraphs)\n",
    "print(\"Extracting and processing relevant sections...\")\n",
    "all_relevant_sections = []\n",
    "extraction_start_time = time.time()\n",
    "for permco, calls in ecc_sample.items():\n",
    "    for call_id, value in calls.items():\n",
    "        company_info = value.get('company_name', '')\n",
    "        date = value.get('date', '')\n",
    "        text = value.get('text_content', '')\n",
    "        relevant_sections = text_processor.extract_and_split_section(permco, call_id, company_info, date, text)\n",
    "        all_relevant_sections.extend(relevant_sections)\n",
    "        # Add the relevant sections to the ECC sample\n",
    "        value['relevant_sections'] = relevant_sections\n",
    "extraction_end_time = time.time()\n",
    "print(f\"Extraction and processing completed in {extraction_end_time - extraction_start_time:.2f} seconds.\")\n",
    "\n",
    "if not all_relevant_sections:\n",
    "    print(\"No relevant sections found to transform with BERTopic.\")\n",
    "else:\n",
    "    # Step 3: Load the embedding model and BERTopic model, compute embeddings, and transform documents\n",
    "    # Determine the computation device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the SentenceTransformer embedding model\n",
    "    print(f\"Loading SentenceTransformer model: {embedding_model_choice}...\")\n",
    "    embedding_model = SentenceTransformer(embedding_model_choice, device=device)\n",
    "    print(\"SentenceTransformer embedding model loaded successfully.\")\n",
    "\n",
    "    # Load the pre-trained BERTopic model\n",
    "    print(f\"Loading BERTopic model from {model_load_path}...\")\n",
    "    topic_model = BERTopic.load(model_load_path, embedding_model=embedding_model)\n",
    "    print(\"BERTopic model loaded successfully.\")\n",
    "\n",
    "    # Compute embeddings for the new documents\n",
    "    print(\"Computing embeddings for new documents...\")\n",
    "    embeddings_start_time = time.time()\n",
    "    embeddings = embedding_model.encode(all_relevant_sections, show_progress_bar=True)\n",
    "    embeddings_end_time = time.time()\n",
    "    print(f\"Computed embeddings for {len(all_relevant_sections)} documents in {embeddings_end_time - embeddings_start_time:.2f} seconds.\")\n",
    "\n",
    "    # Transform documents to get topic assignments and probabilities\n",
    "    print(\"Transforming documents with the BERTopic model...\")\n",
    "    transform_start_time = time.time()\n",
    "    topics, probabilities = topic_model.transform(all_relevant_sections, embeddings)\n",
    "    transform_end_time = time.time()\n",
    "    print(f\"Transformed documents in {transform_end_time - transform_start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Hierarchical topics\u001b[39;00m\n\u001b[0;32m      5\u001b[0m linkage_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: sch\u001b[38;5;241m.\u001b[39mlinkage(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m'\u001b[39m, optimal_ordering\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m hierarchical_topics \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhierarchical_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_relevant_sections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinkage_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinkage_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikla\\miniconda3\\envs\\bert_ma\\Lib\\site-packages\\bertopic\\_bertopic.py:1090\u001b[0m, in \u001b[0;36mBERTopic.hierarchical_topics\u001b[1;34m(self, docs, use_ctfidf, linkage_function, distance_function)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     Z[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m get_unique_distances(Z[:, \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# Calculate basic bag-of-words to be iteratively merged later\u001b[39;00m\n\u001b[1;32m-> 1090\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopics_\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1091\u001b[0m documents_per_topic \u001b[38;5;241m=\u001b[39m documents\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m], as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin})\n\u001b[0;32m   1092\u001b[0m documents_per_topic \u001b[38;5;241m=\u001b[39m documents_per_topic\u001b[38;5;241m.\u001b[39mloc[documents_per_topic\u001b[38;5;241m.\u001b[39mTopic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\nikla\\miniconda3\\envs\\bert_ma\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\nikla\\miniconda3\\envs\\bert_ma\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikla\\miniconda3\\envs\\bert_ma\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\nikla\\miniconda3\\envs\\bert_ma\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from scipy.cluster import hierarchy as sch\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Hierarchical topics\n",
    "linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)\n",
    "hierarchical_topics = topic_model.hierarchical_topics(all_relevant_sections, linkage_function=linkage_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
